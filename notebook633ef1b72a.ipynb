{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11673405,"sourceType":"datasetVersion","datasetId":7326177}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.623192Z","iopub.execute_input":"2025-05-04T15:52:16.623548Z","iopub.status.idle":"2025-05-04T15:52:16.627463Z","shell.execute_reply.started":"2025-05-04T15:52:16.623526Z","shell.execute_reply":"2025-05-04T15:52:16.626628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/predict-u-s-engineering-salaries/train.csv')\ntest_df = pd.read_csv('/kaggle/input/predict-u-s-engineering-salaries/test.csv')\nsolution_format = pd.read_csv(\"/kaggle/input/predict-u-s-engineering-salaries/solution_format.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.631363Z","iopub.execute_input":"2025-05-04T15:52:16.631563Z","iopub.status.idle":"2025-05-04T15:52:16.815253Z","shell.execute_reply.started":"2025-05-04T15:52:16.631548Z","shell.execute_reply":"2025-05-04T15:52:16.814651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution_format.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.816496Z","iopub.execute_input":"2025-05-04T15:52:16.816996Z","iopub.status.idle":"2025-05-04T15:52:16.823570Z","shell.execute_reply.started":"2025-05-04T15:52:16.816976Z","shell.execute_reply":"2025-05-04T15:52:16.822867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.824328Z","iopub.execute_input":"2025-05-04T15:52:16.824578Z","iopub.status.idle":"2025-05-04T15:52:16.852878Z","shell.execute_reply.started":"2025-05-04T15:52:16.824556Z","shell.execute_reply":"2025-05-04T15:52:16.852171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.854317Z","iopub.execute_input":"2025-05-04T15:52:16.854991Z","iopub.status.idle":"2025-05-04T15:52:16.880538Z","shell.execute_reply.started":"2025-05-04T15:52:16.854969Z","shell.execute_reply":"2025-05-04T15:52:16.879857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.salary_category.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.881199Z","iopub.execute_input":"2025-05-04T15:52:16.881439Z","iopub.status.idle":"2025-05-04T15:52:16.897304Z","shell.execute_reply.started":"2025-05-04T15:52:16.881423Z","shell.execute_reply":"2025-05-04T15:52:16.896698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.job_state.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.897919Z","iopub.execute_input":"2025-05-04T15:52:16.898126Z","iopub.status.idle":"2025-05-04T15:52:16.914045Z","shell.execute_reply.started":"2025-05-04T15:52:16.898111Z","shell.execute_reply":"2025-05-04T15:52:16.913490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(train_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.914621Z","iopub.execute_input":"2025-05-04T15:52:16.914840Z","iopub.status.idle":"2025-05-04T15:52:16.933482Z","shell.execute_reply.started":"2025-05-04T15:52:16.914806Z","shell.execute_reply":"2025-05-04T15:52:16.932757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(test_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.934208Z","iopub.execute_input":"2025-05-04T15:52:16.934391Z","iopub.status.idle":"2025-05-04T15:52:16.950932Z","shell.execute_reply.started":"2025-05-04T15:52:16.934377Z","shell.execute_reply":"2025-05-04T15:52:16.950281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.951872Z","iopub.execute_input":"2025-05-04T15:52:16.952134Z","iopub.status.idle":"2025-05-04T15:52:16.974700Z","shell.execute_reply.started":"2025-05-04T15:52:16.952114Z","shell.execute_reply":"2025-05-04T15:52:16.974131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:16.976388Z","iopub.execute_input":"2025-05-04T15:52:16.976925Z","iopub.status.idle":"2025-05-04T15:52:17.322620Z","shell.execute_reply.started":"2025-05-04T15:52:16.976907Z","shell.execute_reply":"2025-05-04T15:52:17.321862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dict(train_df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:17.323437Z","iopub.execute_input":"2025-05-04T15:52:17.324240Z","iopub.status.idle":"2025-05-04T15:52:17.337301Z","shell.execute_reply.started":"2025-05-04T15:52:17.324216Z","shell.execute_reply":"2025-05-04T15:52:17.336587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn==1.5.0 rich==13.7.1 imbalanced-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:52:17.337943Z","iopub.execute_input":"2025-05-04T15:52:17.338181Z","iopub.status.idle":"2025-05-04T15:52:20.348282Z","shell.execute_reply.started":"2025-05-04T15:52:17.338160Z","shell.execute_reply":"2025-05-04T15:52:20.347539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/predict-u-s-engineering-salaries/train.csv')\ntest_df = pd.read_csv('/kaggle/input/predict-u-s-engineering-salaries/test.csv')\n\n# Define categorical, numerical, and boolean columns\ncategorical_cols = ['job_title', 'job_state', 'feature_1', 'job_posted_date']\nnumerical_cols = ['feature_2'] + [f'job_desc_{i:03d}' for i in range(1, 301)]\nboolean_cols = ['feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', \n                'feature_8', 'feature_9', 'feature_11', 'feature_12']\n\n# Handle missing values\n# Impute job_state with most frequent value\nstate_imputer = SimpleImputer(strategy='most_frequent')\ntrain_df['job_state'] = state_imputer.fit_transform(train_df[['job_state']]).flatten()\ntest_df['job_state'] = state_imputer.transform(test_df[['job_state']]).flatten()\n\n# Impute feature_10 with median\nfeature10_imputer = SimpleImputer(strategy='median')\ntrain_df['feature_10'] = feature10_imputer.fit_transform(train_df[['feature_10']]).flatten()\ntest_df['feature_10'] = feature10_imputer.transform(test_df[['feature_10']]).flatten()\n\n# Drop row with missing job_posted_date\ntrain_df = train_df.dropna(subset=['job_posted_date'])\n\n# Feature engineering: Extract year and month from job_posted_date before encoding\ndef extract_year(date_str):\n    try:\n        return int(str(date_str)[:4])\n    except (ValueError, TypeError):\n        return np.nan\n\ndef extract_month(date_str):\n    try:\n        month_str = str(date_str)[5:]\n        return int(month_str) if month_str else np.nan\n    except (ValueError, TypeError):\n        return np.nan\n\ntrain_df['job_posted_year'] = train_df['job_posted_date'].apply(extract_year)\ntrain_df['job_posted_month'] = train_df['job_posted_date'].apply(extract_month)\ntest_df['job_posted_year'] = test_df['job_posted_date'].apply(extract_year)\ntest_df['job_posted_month'] = test_df['job_posted_date'].apply(extract_month)\n\n# Impute any missing year/month values with median\nyear_imputer = SimpleImputer(strategy='median')\nmonth_imputer = SimpleImputer(strategy='median')\ntrain_df[['job_posted_year', 'job_posted_month']] = year_imputer.fit_transform(train_df[['job_posted_year', 'job_posted_month']])\ntest_df[['job_posted_year', 'job_posted_month']] = year_imputer.transform(test_df[['job_posted_year', 'job_posted_month']])\n\n# Additional feature engineering\n# Interaction feature\ntrain_df['feature_2_times_10'] = train_df['feature_2'] * train_df['feature_10']\ntest_df['feature_2_times_10'] = test_df['feature_2'] * test_df['feature_10']\nnumerical_cols.append('feature_2_times_10')\n\n# Polynomial features\ntrain_df['feature_2_squared'] = train_df['feature_2'] ** 2\ntest_df['feature_2_squared'] = test_df['feature_2'] ** 2\ntrain_df['feature_10_squared'] = train_df['feature_10'] ** 2\ntest_df['feature_10_squared'] = test_df['feature_10'] ** 2\nnumerical_cols.extend(['feature_2_squared', 'feature_10_squared'])\n\n# Categorical interaction\ntrain_df['title_state_interaction'] = train_df['job_title'].astype(str) + '_' + train_df['job_state'].astype(str)\ntest_df['title_state_interaction'] = test_df['job_title'].astype(str) + '_' + test_df['job_state'].astype(str)\ncategorical_cols.append('title_state_interaction')\n\n# Encode categorical variables\nlabel_encoders = {}\nfor col in categorical_cols:\n    all_values = pd.concat([train_df[col], test_df[col]]).unique()\n    le = LabelEncoder()\n    le.fit(all_values)\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n    label_encoders[col] = le\n\n# Convert boolean columns to integers\nfor col in boolean_cols:\n    train_df[col] = train_df[col].astype(int)\n    test_df[col] = test_df[col].astype(int)\n\n# Scale numerical features\nscaler = StandardScaler()\ntrain_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\ntest_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n\n# Dimensionality reduction with PCA on job_desc_* columns\njob_desc_cols = [f'job_desc_{i:03d}' for i in range(1, 301)]\npca = PCA(n_components=150)  # Increased components\ntrain_job_desc_pca = pca.fit_transform(train_df[job_desc_cols])\ntest_job_desc_pca = pca.transform(test_df[job_desc_cols])\n\n# Create DataFrames for PCA components\ntrain_pca_df = pd.DataFrame(train_job_desc_pca, columns=[f'job_desc_pca_{i+1}' for i in range(150)], index=train_df.index)\ntest_pca_df = pd.DataFrame(test_job_desc_pca, columns=[f'job_desc_pca_{i+1}' for i in range(150)], index=test_df.index)\n\n# Concatenate PCA components to avoid fragmentation\ntrain_df = pd.concat([train_df.drop(columns=job_desc_cols), train_pca_df], axis=1)\ntest_df = pd.concat([test_df.drop(columns=job_desc_cols), test_pca_df], axis=1)\n\n# Prepare target variable\ntarget_encoder = LabelEncoder()\ntrain_df['salary_category'] = target_encoder.fit_transform(train_df['salary_category'])\n\n# Prepare features and target\nX = train_df.drop(columns=['obs', 'salary_category'])\ny = train_df['salary_category']\nX_test = test_df.drop(columns=['obs'])\n\n# Apply SMOTE to handle class imbalance\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Split data into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\nX_val, X_test_internal, y_val, y_test_internal = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n\n\n# Custom Dataset class (unchanged)\nclass SalaryDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X.values, dtype=torch.float32)\n        self.y = torch.tensor(y.values, dtype=torch.long) if y is not None else None\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\n# Define the neural network (unchanged)\nclass SalaryClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size=1024, num_classes=3, dropout_rate=0.2):\n        super(SalaryClassifier, self).__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.swish1 = nn.SiLU()\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n        self.swish2 = nn.SiLU()\n        self.dropout2 = nn.Dropout(dropout_rate)\n        \n        self.residual_adapter1 = nn.Linear(hidden_size, hidden_size // 2)\n        \n        self.layer3 = nn.Linear(hidden_size // 2, hidden_size // 4)\n        self.bn3 = nn.BatchNorm1d(hidden_size // 4)\n        self.relu3 = nn.ReLU()\n        self.dropout3 = nn.Dropout(dropout_rate)\n        \n        self.layer4 = nn.Linear(hidden_size // 4, hidden_size // 8)\n        self.bn4 = nn.BatchNorm1d(hidden_size // 8)\n        self.relu4 = nn.ReLU()\n        self.dropout4 = nn.Dropout(dropout_rate)\n        \n        self.residual_adapter2 = nn.Linear(hidden_size // 4, hidden_size // 8)\n        \n        self.layer5 = nn.Linear(hidden_size // 8, hidden_size // 16)\n        self.bn5 = nn.BatchNorm1d(hidden_size // 16)\n        self.relu5 = nn.ReLU()\n        self.dropout5 = nn.Dropout(dropout_rate)\n        \n        self.layer6 = nn.Linear(hidden_size // 16, num_classes)\n    \n    def forward(self, x):\n        x1 = self.layer1(x)\n        x1 = self.bn1(x1)\n        x1 = self.swish1(x1)\n        x1 = self.dropout1(x1)\n        \n        x2 = self.layer2(x1)\n        x2 = self.bn2(x2)\n        x2 = self.swish2(x2)\n        x2 = self.dropout2(x2)\n        residual1 = self.residual_adapter1(x1)\n        x2 = x2 + residual1\n        \n        x3 = self.layer3(x2)\n        x3 = self.bn3(x3)\n        x3 = self.relu3(x3)\n        x3 = self.dropout3(x3)\n        \n        x4 = self.layer4(x3)\n        x4 = self.bn4(x4)\n        x4 = self.relu4(x4)\n        x4 = self.dropout4(x4)\n        residual2 = self.residual_adapter2(x3)\n        x4 = x4 + residual2\n        \n        x5 = self.layer5(x4)\n        x5 = self.bn5(x5)\n        x5 = self.relu5(x5)\n        x5 = self.dropout5(x5)\n        \n        x6 = self.layer6(x5)\n        return x6\n\n# Corrected PyTorchClassifier wrapper\nclass PyTorchClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, input_size, hidden_size=1024, num_classes=3, dropout_rate=0.2, \n                 learning_rate=0.00003, batch_size=128, num_epochs=50, weight_decay=1e-4):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.dropout_rate = dropout_rate\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n        self.weight_decay = weight_decay\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = None\n        self.classes_ = None  # Initialize classes_ attribute\n    \n    def fit(self, X, y):\n        # Store class labels\n        self.classes_ = np.unique(y)\n        \n        # Initialize model\n        self.model = SalaryClassifier(\n            input_size=self.input_size, \n            hidden_size=self.hidden_size, \n            num_classes=self.num_classes, \n            dropout_rate=self.dropout_rate\n        ).to(self.device)\n        \n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n        \n        # Create DataLoader\n        dataset = SalaryDataset(pd.DataFrame(X), pd.Series(y))\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n        \n        # Training loop\n        self.model.train()\n        for epoch in range(self.num_epochs):\n            epoch_loss = 0\n            for X_batch, y_batch in dataloader:\n                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n                optimizer.zero_grad()\n                outputs = self.model(X_batch)\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # Corrected\n                optimizer.step()\n                epoch_loss += loss.item()\n            \n            epoch_loss /= len(dataloader)\n            scheduler.step(epoch_loss)\n        \n        return self\n    \n    def predict(self, X):\n        self.model.eval()\n        predictions = []\n        dataset = SalaryDataset(pd.DataFrame(X))\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n        \n        with torch.no_grad():\n            for X_batch in dataloader:\n                X_batch = X_batch.to(self.device)\n                outputs = self.model(X_batch)\n                _, predicted = torch.max(outputs.data, 1)\n                predictions.extend(predicted.cpu().numpy())\n        \n        return np.array(predictions)\n    \n    def predict_proba(self, X):\n        self.model.eval()\n        probabilities = []\n        dataset = SalaryDataset(pd.DataFrame(X))\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n        \n        with torch.no_grad():\n            for X_batch in dataloader:\n                X_batch = X_batch.to(self.device)\n                outputs = self.model(X_batch)\n                probs = torch.softmax(outputs, dim=1)\n                probabilities.extend(probs.cpu().numpy())\n        \n        return np.array(probabilities)\n\n# Define hyperparameter grid\nparam_grid = {\n    'hidden_size': [512, 1024],\n    'dropout_rate': [0.2, 0.3],\n    'learning_rate': [0.00003, 0.0001],\n    'batch_size': [64, 128]\n}\n\n# Initialize the PyTorch classifier\nbase_model = PyTorchClassifier(\n    input_size=X.shape[1], \n    num_classes=len(target_encoder.classes_), \n    num_epochs=50\n)\n\n# Perform grid search\ngrid_search = GridSearchCV(\n    estimator=base_model,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=1,\n    verbose=2,\n    error_score='raise'\n)\n\n# Fit grid search\ngrid_search.fit(X_resampled, y_resampled)\n\n# Print best parameters and score\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best cross-validation accuracy:\", grid_search.best_score_)\n\n# Train the best model on full training data\nbest_model = grid_search.best_estimator_\nbest_model.fit(X_resampled, y_resampled)\n\n# Evaluate on internal test set\ntest_predictions = best_model.predict(X_test_internal)\ntest_accuracy = (test_predictions == y_test_internal).mean() * 100\nprint(f'Internal Test Accuracy with Best Model: {test_accuracy:.2f}%')\n\n# Predict on test set\npredictions = best_model.predict(X_test)\npredictions = target_encoder.inverse_transform(predictions)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'obs': test_df['obs'],\n    'salary_category': predictions\n})\n\n# Save submission\nsubmission.to_csv('submission_grid_search.csv', index=False)\n\n# Display first few predictions\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:37:07.527545Z","iopub.execute_input":"2025-05-04T16:37:07.528380Z","iopub.status.idle":"2025-05-04T16:39:23.918065Z","shell.execute_reply.started":"2025-05-04T16:37:07.528354Z","shell.execute_reply":"2025-05-04T16:39:23.917386Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 16 candidates, totalling 48 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=512, learning_rate=3e-05; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=512, learning_rate=3e-05; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=512, learning_rate=3e-05; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=512, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=512, learning_rate=0.0001; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=512, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=1024, learning_rate=3e-05; total time=   3.6s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=1024, learning_rate=3e-05; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=1024, learning_rate=3e-05; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=1024, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=1024, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.2, hidden_size=1024, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=512, learning_rate=3e-05; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=512, learning_rate=3e-05; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=512, learning_rate=3e-05; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=512, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=512, learning_rate=0.0001; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=512, learning_rate=0.0001; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=1024, learning_rate=3e-05; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=1024, learning_rate=3e-05; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=1024, learning_rate=3e-05; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=1024, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=1024, learning_rate=0.0001; total time=   3.3s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=64, dropout_rate=0.3, hidden_size=1024, learning_rate=0.0001; total time=   3.4s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=512, learning_rate=3e-05; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=512, learning_rate=3e-05; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=512, learning_rate=3e-05; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=512, learning_rate=0.0001; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=512, learning_rate=0.0001; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=512, learning_rate=0.0001; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=1024, learning_rate=3e-05; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=1024, learning_rate=3e-05; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=1024, learning_rate=3e-05; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=1024, learning_rate=0.0001; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=1024, learning_rate=0.0001; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.2, hidden_size=1024, learning_rate=0.0001; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=512, learning_rate=3e-05; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=512, learning_rate=3e-05; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=512, learning_rate=3e-05; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=512, learning_rate=0.0001; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=512, learning_rate=0.0001; total time=   1.8s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=512, learning_rate=0.0001; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=1024, learning_rate=3e-05; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=1024, learning_rate=3e-05; total time=   2.0s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=1024, learning_rate=3e-05; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=1024, learning_rate=0.0001; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=1024, learning_rate=0.0001; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END batch_size=128, dropout_rate=0.3, hidden_size=1024, learning_rate=0.0001; total time=   1.9s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Best parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'hidden_size': 1024, 'learning_rate': 3e-05}\nBest cross-validation accuracy: 0.6773120425815037\nInternal Test Accuracy with Best Model: 85.84%\n    obs salary_category\n0  1281            High\n1  1282          Medium\n2  1283            High\n3  1284             Low\n4  1285            High\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}